# -*- coding: utf-8 -*-
"""generativeai_project1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OAn6yT1ENXOKLSUQCnGHGFqRUVuVFBgC
"""

!pip install transformers datasets accelerate torch

import os
os.environ["WANDB_DISABLED"] = "true"  # Completely disables W&B

# Small dataset ~500 lines
text = """Artificial intelligence is transforming modern industries.
Machine learning models learn patterns from data.
AI systems improve decision making.
Deep learning enhances model accuracy.
Natural language processing enables human communication.
AI supports innovation across sectors.
Automation increases productivity.
Predictive analytics forecasts trends.
AI improves healthcare outcomes.
Technology drives digital transformation.
""" * 50  # Multiply to ~500 lines

with open("data.txt", "w", encoding="utf-8") as f:
    f.write(text)

print("data.txt created successfully")

from datasets import load_dataset

dataset = load_dataset(
    "text",
    data_files={"train": "data.txt"}
)

print(dataset)

from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # important for padding

# Load GPT-2 model with language modeling head
model = GPT2LMHeadModel.from_pretrained("gpt2")

print("GPT-2 model and tokenizer loaded successfully!")

def tokenize_function(examples):
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )
    tokenized["labels"] = tokenized["input_ids"].copy()  # Needed for GPT-2 loss
    return tokenized

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"]
)

print("Tokenization complete with labels!")

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=2,
    logging_steps=50,
    save_steps=500,
    fp16=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"]
)

trainer.train()

prompt = "Artificial intelligence is"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_length=80,
    temperature=0.7,
    top_k=50,
    top_p=0.95
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

model.save_pretrained("my_finetuned_gpt2")
tokenizer.save_pretrained("my_finetuned_gpt2")